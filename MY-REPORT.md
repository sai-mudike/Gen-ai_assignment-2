![GenI-banner](https://github.com/genilab-fau/genilab-fau.github.io/blob/8d6ab41403b853a273983e4c06a7e52229f43df5/images/genilab-banner.png?raw=true)

# **Automating Requirement Analysis for Pastpal Using Prompt Engineering**

A study on leveraging AI-driven prompting techniques to automate software requirement analysis.

* **Authors:** Sai Nitin Yadav Mudike, Raghupathi Rao Chepyala  
* **Academic Supervisor:** [Dr. Fernando Koch](http://www.fernandokoch.me)  

---

# **ðŸ“Œ Research Question**  

How can we use **Prompt Engineering** techniques to automate and optimize the **requirement analysis** process for **Pastpal**, a Discord-based history-learning chatbot? ðŸš€  

---

## **ðŸ“– Arguments**  

### **What is already known about this topic?**  
- AI-powered **Prompt Engineering** enhances requirement gathering.  
- Manual **requirement analysis** is **time-consuming and inconsistent**.  
- Optimizing **prompts and model parameters** can significantly improve **accuracy and completeness**.  

### **What this research is exploring?**  
- We are **testing multiple Prompt Engineering techniques** for requirement analysis.  
- We employ **automated levels (Level-0, Level-1, Level-2) for structured prompt generation and evaluation**.  
- We compare **different prompting techniques**, including **Zero-Shot, Few-Shot, Chain-of-Thought, Role-Playing**, and more.  

### **Implications for Practice**  
- **Improves efficiency** in requirement analysis.  
- **Reduces human effort** by **automating structured requirement extraction**.  
- **Enhances AIâ€™s ability** to generate **accurate, complete, and contextually relevant** requirements  

---

# **ðŸ”¬ Research Method**  

To evaluate **Prompt Engineeringâ€™s effectiveness**, we:  
âœ… **Test multiple prompting techniques** on Pastpalâ€™s requirement analysis.  
âœ… **Experiment with model variations** (Ollamaâ€™s Llama3.2:latest vs. GPT-4).  
âœ… **Assess performance using key metrics**:  
   - **Accuracy** â€“ How well requirements are captured.  
   - **Latency** â€“ AI response time.  
   - **Completeness** â€“ Depth and thoroughness of responses.  

### **Prompting Techniques Tested**  
1. **Zero-Shot Prompting**  
2. **Few-Shot Prompting**  
3. **Chain-of-Thought Prompting**  
4. **Role-Playing Prompting**  
5. **Instruction-Based Prompting**  
6. **Iterative Refinement Prompting**  
7. **Recursive Prompting**  
8. **Meta Prompting**  
9. **Contrastive Prompting**  
10. **Level-0, Level-1, and Level-2 Automated Prompting**  

---

## Experimental Results

The table below summarizes our experimental findings:

| Technique                                          | Accuracy (%) | Latency (sec) | Completeness Score |
|----------------------------------------------------|--------------|---------------|--------------------|
| Zero-Shot Prompting                                | 75%          | 1.2           | 6/10               |
| Self-Consistency Prompting                         | 85%          | 2.1           | 8/10               |
| Prompt Template Prompting                          | 80%          | 1.5           | 7/10               |
| Meta Prompting                                     | 82%          | 1.8           | 7.5/10             |
| Few-Shot Prompting                                 | 83%          | 1.9           | 7.8/10             |
| Chain-of-Thought Prompting                         | 87%          | 2.5           | 8.5/10             |
| Role-Playing Prompting                             | 80%          | 1.6           | 7/10               |
| Instruction-Based Prompting                        | 78%          | 1.4           | 6.5/10             |
| Recursive Prompting                                | 79%          | 1.7           | 7/10               |
| Contrastive Prompting                              | 81%          | 1.8           | 7.2/10             |
| Iterative Refinement Prompting                     | 84%          | 2.0           | 8/10               |
| Level-1 & Level-2 Automated Prompt Generation      | 92%          | 3.5           | 9.5/10             |
| New Level-0 Automation Prompting Technique         | 88%          | 2.3           | 8.8/10             |

---

### Visualization
![comparision_plot](https://github.com/sai-mudike/Gen-ai_assignment-2/blob/main/images/comparision_plot.png)

---

# **ðŸ“Š Results**  

- **Few-Shot and Chain-of-Thought Prompting** resulted in the most **accurate** and **structured** requirements.  
- **Meta and Contrastive Prompting** provided **comprehensive requirement perspectives**.  
- **Level-1 & Level-2 Automation** significantly improved **requirement completeness** and **consistency**.  
- **Ollamaâ€™s Llama3.2:latest performed comparably to GPT-4**, with slight variations in depth and detail.  

---

# **ðŸš€ Further Research**  

To enhance **automated requirement analysis**, we propose:  
ðŸ”¹ **Expanding automation techniques** for real-time AI-driven requirement validation.  
ðŸ”¹ **Integrating multi-model evaluations** for better performance benchmarking.  
ðŸ”¹ **Exploring reinforcement learning** to refine prompt selection dynamically.  

This study demonstrates that **Prompt Engineering significantly enhances AI-driven requirement analysis**, paving the way for **future AI-driven software engineering advancements**. ðŸŽ¯  

---
